?search_tweets
# collect twitter data
lq <- search_tweets(
"takbiran", n = 5000, include_rts = TRUE, lang = "id", type = "mixed"
)
View(lq)
tweets <- lq
rm(lq)
df <- tweets
rm(tweets)
# Process each set of tweets into tidy text or corpus objects
tweets.df = df %>% select(text)
tweets.df
?row.names
# ===== SAVE DATA
write.csv(x = tweets.df,
file = '~/Projects/Twitter/SA/tugasakhirfix/Datasets/Data Takbiran.csv',
row.names = FALSE)
tweets.df = read.csv(file = '~/Projects/Twitter/SA/tugasakhirfix/Datasets/Data Takbiran.csv',
header = TRUE,
sep = ',')
# ===== DATA CLEANSING
# Work with Corpus
tweet.corpus = VCorpus(VectorSource(tweets.df$text))
inspect(tweet.corpus[[1]])
View(tweet.corpus)
# Import external function
source(file = '~/Projects/Twitter/SA/tugasakhirfix/Helpers/Function Helper/Cleansing.R')
# TRANSFORM TO LOWER CASE
tweet.corpus = tm_map(tweet.corpus, content_transformer(tolower))
tweet.corpus = tm_map(tweet.corpus, content_transformer(removeURL))
tweet.corpus = tm_map(tweet.corpus, content_transformer(unescapeHTML))
tweet.corpus = tm_map(tweet.corpus, content_transformer(removeMention))
tweet.corpus = tm_map(tweet.corpus, content_transformer(removeCarriage))
tweet.corpus = tm_map(tweet.corpus, content_transformer(removeEmoticon))
tweet.corpus = tm_map(tweet.corpus, content_transformer(removeInvoice))
# Remove additional symbols to white space
tweet.corpus = tm_map(tweet.corpus, toSpace, "[[:punct:]]") # punctuation
tweet.corpus = tm_map(tweet.corpus, toSpace, "[[:digit:]]") # numbers
# Eliminate extra white spaces
tweet.corpus = tm_map(tweet.corpus, stripWhitespace)
# Check the final result
inspect(tweet.corpus[[1]])
spell.lex = read.csv(file = '~/Projects/Twitter/SA/sentimentanalyisi/Helpers/Data Helper/colloquial-indonesian-lexicon.txt',
header = TRUE,
sep = ',',
stringsAsFactors = FALSE)
source(file = '~/Projects/Twitter/SA/sentimentanalyisi/Helpers/Function Helper/Cleansing.R')
tweet.corpus = tm_map(tweet.corpus, spell.correction, spell.lex)
source(file = '~/Projects/Twitter/SA/sentimentanalyisi/Helpers/Function Helper/Cleansing.R')
tweet.corpus = tm_map(tweet.corpus, content_transformer(stemming))
View(tweet.corpus)
# ===== LEXICON-BASED SENTIMENT SCORING
pos = readLines('~/Projects/Twitter/SA/tugasakhirfix/Helpers/Data Helper/s-pos.txt')
neg = readLines('~/Projects/Twitter/SA/tugasakhirfix/Helpers/Data Helper/s-neg.txt')
df.tweet = data.frame(text = sapply(tweet.corpus, as.character),
stringsAsFactors = FALSE)
View(df.tweet)
# Negation Handling
negasi = scan('~/Projects/Twitter/SA/tugasakhirfix/Helpers/Data Helper/negatingword.txt',
what = 'character')
senti = read.csv('~/Projects/Twitter/SA/tugasakhirfix/Helpers/Data Helper/sentiwords_id.txt',
sep = ':',
header = FALSE) %>%
mutate(words = as.character(V1),
score = as.numeric(V2)) %>%
select(c('words','score'))
booster = read.csv('~/Projects/Twitter/SA/tugasakhirfix/Helpers/Data Helper/boosterwords_id.txt',
sep = ":",
header = FALSE) %>%
mutate(words = as.character(V1),
score = as.numeric(V2)) %>%
select(c('words','score'))
source(file = '~/Projects/Twitter/SA/tugasakhirfix/Helpers/Function Helper/Lexicon-Based Scoring Analysis.R')
results = scores.sentiment(df.tweet$text, senti, negasi)
set.seed(2211)
random.tweet = sample(x = df.tweet$text,
size = 10)
head(scores.sentiment(random.tweet, senti, negasi),5)
# Convert score to sentiment classes
results$class = as.factor(ifelse(results$score < 0, 'Negative',
ifelse(results$score == 0, 'Neutral','Positive')))
# ===== SAVE DATA RESULTS
write.csv(x = results,
file = '~/Projects/Twitter/SA/tugasakhirfix/Datasets/Sentiment Takbiran.csv',
row.names = FALSE)
# ===== FILTERING STOPWORDS
rm.stopword = VCorpus(VectorSource(results$text))
# Using edited stopword list
stopwords.id = readLines('~/Projects/Twitter/SA/tugasakhirfix/Helpers/Data Helper/stopwords-id.txt')
rm.stopword = tm_map(rm.stopword, removeWords, stopwords.id)
# Using manually added stopword list
stopwords.id = readLines('~/Projects/Twitter/SA/tugasakhirfix/Helpers/Data Helper/stopwords-manual.txt')
rm.stopword = tm_map(rm.stopword, removeWords, stopwords.id)
# Check the final result
inspect(rm.stopword[[2]])
# Eliminate extra white spaces
rm.stopword = tm_map(rm.stopword, stripWhitespace)
# TOKENIZATION - FREQUENT WORDS
tdm = TermDocumentMatrix(rm.stopword,
control = list(wordLengths = c(1, Inf)))
freq.terms = findFreqTerms(tdm,
lowfreq = 20)
freq.terms[1:50]
# ===== WORDCLOUD
wordcloud(rm.stopword,
max.words = 100,
min.freq = 25,
random.order = FALSE,
colors = brewer.pal(8, "Dark2"))
# ===== BARPLOT OF FREQUENT WORDS
term.freq = rowSums(as.matrix(tdm))
term.freq = subset(term.freq, term.freq >= 350)
df = data.frame(term = names(term.freq),
freq = term.freq)
ggplot(df)+
geom_bar(aes(x = reorder(term,
freq),
y = freq),
stat = 'identity',
fill = I('blue'),
alpha = 0.6)+
labs(title = 'Frekuensi Kata',
subtitle = 'Takbiran',
caption = 'Twitter Crawling 1 Mei 2022')+
xlab('Kata')+
ylab('Jumlah')+
coord_flip()+
theme(axis.text = element_text(size = 9))+
theme_bw()
# ===== BARPLOT OF FREQUENT WORDS
term.freq = rowSums(as.matrix(tdm))
term.freq = subset(term.freq, term.freq >= 350)
df = data.frame(term = names(term.freq),
freq = term.freq)
ggplot(df)+
geom_bar(aes(x = reorder(term,
freq),
y = freq),
stat = 'identity',
fill = I('blue'),
alpha = 0.6)+
labs(title = 'Frekuensi Kata',
subtitle = 'Takbiran',
caption = 'Twitter Crawling 1 Mei 2022')+
xlab('Kata')+
ylab('Jumlah')+
coord_flip()+
theme(axis.text = element_text(size = 9))+
theme_bw()
gc()
View(scores.sentiment)
View(df)
# collect twitter data
df <- search_tweets(
"takbiran", n = 5000, include_rts = TRUE, lang = "id", type = "recent"
)
View(df)
?include_rts
??include_rts
rtweet::search_tweets()
?search_tweets
write.csv(x = df,
file = '~/Projects/Twitter/SA/tugasakhirfix/Datasets/Data Takbiran Mentah.csv',
row.names = FALSE)
flatenned_tweets <- data.frame(lapply(df, as.character), stringsAsFactors = FALSE)
View(flatenned_tweets)
#save raw twitter data
write.csv(x = flatenned_tweets,
file = '~/Projects/Twitter/SA/tugasakhirfix/Datasets/Data Takbiran Mentah.csv',
row.names = FALSE)
# Process each set of tweets into tidy text or corpus objects
tweets.df = df %>% select(text)
tweets.df
#save twitter data
write.csv(x = tweets.df,
file = '~/Projects/Twitter/SA/tugasakhirfix/Datasets/Data Takbiran.csv',
row.names = FALSE)
# Load Data from Computer
tweets.df = read.csv(file = '~/Projects/Twitter/SA/tugasakhirfix/Datasets/Data Takbiran.csv',
header = TRUE,
sep = ',')
# ===== DATA CLEANSING
# Work with Corpus
tweet.corpus = VCorpus(VectorSource(tweets.df$text))
inspect(tweet.corpus[[1]])
# Import external function
source(file = '~/Projects/Twitter/SA/tugasakhirfix/Helpers/Function Helper/Cleansing.R')
# TRANSFORM TO LOWER CASE
tweet.corpus = tm_map(tweet.corpus, content_transformer(tolower))
# FILTERING - CUSTOM CLEANSING FUNCTIONS
tweet.corpus = tm_map(tweet.corpus, content_transformer(removeURL))
tweet.corpus = tm_map(tweet.corpus, content_transformer(unescapeHTML))
tweet.corpus = tm_map(tweet.corpus, content_transformer(removeMention))
tweet.corpus = tm_map(tweet.corpus, content_transformer(removeCarriage))
tweet.corpus = tm_map(tweet.corpus, content_transformer(removeEmoticon))
tweet.corpus = tm_map(tweet.corpus, content_transformer(removeInvoice))
# Remove additional symbols to white space
tweet.corpus = tm_map(tweet.corpus, toSpace, "[[:punct:]]") # punctuation
tweet.corpus = tm_map(tweet.corpus, toSpace, "[[:digit:]]") # numbers
# Eliminate extra white spaces
tweet.corpus = tm_map(tweet.corpus, stripWhitespace)
# Check the final result
inspect(tweet.corpus[[1]])
# SPELLING NORMALIZATION
spell.lex = read.csv(file = '~/Projects/Twitter/SA/tugasakhirfix/Helpers/Data Helper/colloquial-indonesian-lexicon.txt',
header = TRUE,
sep = ',',
stringsAsFactors = FALSE)
# Import external function
source(file = '~/Projects/Twitter/SA/tugasakhirfix/Helpers/Function Helper/Cleansing.R')
tweet.corpus = tm_map(tweet.corpus, spell.correction, spell.lex)
# STEMMING WORDS
# Import external function
source(file = '~/Projects/Twitter/SA/tugasakhirfix/Helpers/Function Helper/Cleansing.R')
tweet.corpus = tm_map(tweet.corpus, content_transformer(stemming))
remove_duplicate <-my_data[!duplicated(tweet.corpus$text), ]
remove_duplicate <-tweet.corpus[!duplicated(tweet.corpus$text), ]
remove_duplicate <-tweet.corpus[!duplicated(tweet.corpus$text)]
View(remove_duplicate)
remove_duplicate <-df[!duplicated(df$text)]
View(df)
remove_duplicate <-df[!duplicated(df$full_text)]
View(df)
df %>% distinct(full_text, .keep_all = TRUE)
df %>% distinct(full_text, .keep_all = FALSE)
dfc = df %>% distinct(full_text, .keep_all = FALSE)
View(dfc)
View(dfc)
View(df)
# remove duplicate
dfc = df %>% distinct(text, .keep_all = FALSE)
View(dfc)
# remove duplicate
tweets.df = df %>% distinct(text, .keep_all = FALSE)
#save twitter data
write.csv(x = tweets.df,
file = '~/Projects/Twitter/SA/tugasakhirfix/Datasets/Data Takbiran.csv',
row.names = FALSE)
# Load Data from Computer
tweets.df = read.csv(file = '~/Projects/Twitter/SA/tugasakhirfix/Datasets/Data Takbiran.csv',
header = TRUE,
sep = ',')
# ===== DATA CLEANSING
# Work with Corpus
tweet.corpus = VCorpus(VectorSource(tweets.df$text))
inspect(tweet.corpus[[1]])
# Import external function
source(file = '~/Projects/Twitter/SA/tugasakhirfix/Helpers/Function Helper/Cleansing.R')
# TRANSFORM TO LOWER CASE
tweet.corpus = tm_map(tweet.corpus, content_transformer(tolower))
# FILTERING - CUSTOM CLEANSING FUNCTIONS
tweet.corpus = tm_map(tweet.corpus, content_transformer(removeURL))
tweet.corpus = tm_map(tweet.corpus, content_transformer(unescapeHTML))
tweet.corpus = tm_map(tweet.corpus, content_transformer(removeMention))
tweet.corpus = tm_map(tweet.corpus, content_transformer(removeCarriage))
tweet.corpus = tm_map(tweet.corpus, content_transformer(removeEmoticon))
tweet.corpus = tm_map(tweet.corpus, content_transformer(removeInvoice))
# Remove additional symbols to white space
tweet.corpus = tm_map(tweet.corpus, toSpace, "[[:punct:]]") # punctuation
tweet.corpus = tm_map(tweet.corpus, toSpace, "[[:digit:]]") # numbers
# Eliminate extra white spaces
tweet.corpus = tm_map(tweet.corpus, stripWhitespace)
# Check the final result
inspect(tweet.corpus[[1]])
# SPELLING NORMALIZATION
spell.lex = read.csv(file = '~/Projects/Twitter/SA/tugasakhirfix/Helpers/Data Helper/colloquial-indonesian-lexicon.txt',
header = TRUE,
sep = ',',
stringsAsFactors = FALSE)
# Import external function
source(file = '~/Projects/Twitter/SA/tugasakhirfix/Helpers/Function Helper/Cleansing.R')
tweet.corpus = tm_map(tweet.corpus, spell.correction, spell.lex)
# Import external function
source(file = '~/Projects/Twitter/SA/tugasakhirfix/Helpers/Function Helper/Cleansing.R')
tweet.corpus = tm_map(tweet.corpus, content_transformer(stemming))
# ===== LEXICON-BASED SENTIMENT SCORING
pos = readLines('~/Projects/Twitter/SA/tugasakhirfix/Helpers/Data Helper/s-pos.txt')
neg = readLines('~/Projects/Twitter/SA/tugasakhirfix/Helpers/Data Helper/s-neg.txt')
df.tweet = data.frame(text = sapply(tweet.corpus, as.character),
stringsAsFactors = FALSE)
# Negation Handling
negasi = scan('~/Projects/Twitter/SA/tugasakhirfix/Helpers/Data Helper/negatingword.txt',
what = 'character')
senti = read.csv('~/Projects/Twitter/SA/tugasakhirfix/Helpers/Data Helper/sentiwords_id.txt',
sep = ':',
header = FALSE) %>%
mutate(words = as.character(V1),
score = as.numeric(V2)) %>%
select(c('words','score'))
booster = read.csv('~/Projects/Twitter/SA/tugasakhirfix/Helpers/Data Helper/boosterwords_id.txt',
sep = ":",
header = FALSE) %>%
mutate(words = as.character(V1),
score = as.numeric(V2)) %>%
select(c('words','score'))
# Import external function
source(file = '~/Projects/Twitter/SA/tugasakhirfix/Helpers/Function Helper/Lexicon-Based Scoring Analysis.R')
results = scores.sentiment(df.tweet$text, senti, negasi)
set.seed(2211)
random.tweet = sample(x = df.tweet$text,
size = 10)
head(scores.sentiment(random.tweet, senti, negasi),5)
results$class = as.factor(ifelse(results$score < 0, 'Negative',
ifelse(results$score == 0, 'Neutral','Positive')))
# ===== SAVE DATA RESULTS
write.csv(x = results,
file = '~/Projects/Twitter/SA/tugasakhirfix/Datasets/Sentiment Takbiran.csv',
row.names = FALSE)
# ===== FILTERING STOPWORDS
rm.stopword = VCorpus(VectorSource(results$text))
# Using edited stopword list
stopwords.id = readLines('~/Projects/Twitter/SA/tugasakhirfix/Helpers/Data Helper/stopwords-id.txt')
rm.stopword = tm_map(rm.stopword, removeWords, stopwords.id)
# Using manually added stopword list
stopwords.id = readLines('~/Projects/Twitter/SA/tugasakhirfix/Helpers/Data Helper/stopwords-manual.txt')
rm.stopword = tm_map(rm.stopword, removeWords, stopwords.id)
# Check the final result
inspect(rm.stopword[[2]])
# Eliminate extra white spaces
rm.stopword = tm_map(rm.stopword, stripWhitespace)
# TOKENIZATION - FREQUENT WORDS
tdm = TermDocumentMatrix(rm.stopword,
control = list(wordLengths = c(1, Inf)))
freq.terms = findFreqTerms(tdm,
lowfreq = 20)
freq.terms[1:50]
# ===== WORDCLOUD
wordcloud(rm.stopword,
max.words = 100,
min.freq = 25,
random.order = FALSE,
colors = brewer.pal(8, "Dark2"))
# ===== BARPLOT OF FREQUENT WORDS
term.freq = rowSums(as.matrix(tdm))
term.freq = subset(term.freq, term.freq >= 350)
df = data.frame(term = names(term.freq),
freq = term.freq)
ggplot(df)+
geom_bar(aes(x = reorder(term,
freq),
y = freq),
stat = 'identity',
fill = I('blue'),
alpha = 0.6)+
labs(title = 'Frekuensi Kata',
subtitle = 'Takbiran',
caption = 'Twitter Crawling 1 Mei 2022')+
xlab('Kata')+
ylab('Jumlah')+
coord_flip()+
theme(axis.text = element_text(size = 9))+
theme_bw()
# ===== BARPLOT OF FREQUENT WORDS
term.freq = rowSums(as.matrix(tdm))
term.freq = subset(term.freq, term.freq >= 100)
df = data.frame(term = names(term.freq),
freq = term.freq)
ggplot(df)+
geom_bar(aes(x = reorder(term,
freq),
y = freq),
stat = 'identity',
fill = I('blue'),
alpha = 0.6)+
labs(title = 'Frekuensi Kata',
subtitle = 'Takbiran',
caption = 'Twitter Crawling 1 Mei 2022')+
xlab('Kata')+
ylab('Jumlah')+
coord_flip()+
theme(axis.text = element_text(size = 9))+
theme_bw()
# Using manually added stopword list
stopwords.id = readLines('~/Projects/Twitter/SA/tugasakhirfix/Helpers/Data Helper/stopwords-manual.txt')
rm.stopword = tm_map(rm.stopword, removeWords, stopwords.id)
# Check the final result
inspect(rm.stopword[[2]])
# Eliminate extra white spaces
rm.stopword = tm_map(rm.stopword, stripWhitespace)
# TOKENIZATION - FREQUENT WORDS
tdm = TermDocumentMatrix(rm.stopword,
control = list(wordLengths = c(1, Inf)))
freq.terms = findFreqTerms(tdm,
lowfreq = 20)
freq.terms[1:50]
# ===== WORDCLOUD
wordcloud(rm.stopword,
max.words = 100,
min.freq = 25,
random.order = FALSE,
colors = brewer.pal(8, "Dark2"))
# ===== BARPLOT OF FREQUENT WORDS
term.freq = rowSums(as.matrix(tdm))
term.freq = subset(term.freq, term.freq >= 100)
df = data.frame(term = names(term.freq),
freq = term.freq)
ggplot(df)+
geom_bar(aes(x = reorder(term,
freq),
y = freq),
stat = 'identity',
fill = I('blue'),
alpha = 0.6)+
labs(title = 'Frekuensi Kata',
subtitle = 'Takbiran',
caption = 'Twitter Crawling 1 Mei 2022')+
xlab('Kata')+
ylab('Jumlah')+
coord_flip()+
theme(axis.text = element_text(size = 9))+
theme_bw()
# Using manually added stopword list
stopwords.id = readLines('~/Projects/Twitter/SA/tugasakhirfix/Helpers/Data Helper/stopwords-manual.txt')
rm.stopword = tm_map(rm.stopword, removeWords, stopwords.id)
# Check the final result
inspect(rm.stopword[[2]])
# Eliminate extra white spaces
rm.stopword = tm_map(rm.stopword, stripWhitespace)
# TOKENIZATION - FREQUENT WORDS
tdm = TermDocumentMatrix(rm.stopword,
control = list(wordLengths = c(1, Inf)))
freq.terms = findFreqTerms(tdm,
lowfreq = 20)
freq.terms[1:50]
# ===== WORDCLOUD
wordcloud(rm.stopword,
max.words = 100,
min.freq = 25,
random.order = FALSE,
colors = brewer.pal(8, "Dark2"))
# ===== BARPLOT OF FREQUENT WORDS
term.freq = rowSums(as.matrix(tdm))
term.freq = subset(term.freq, term.freq >= 100)
df = data.frame(term = names(term.freq),
freq = term.freq)
ggplot(df)+
geom_bar(aes(x = reorder(term,
freq),
y = freq),
stat = 'identity',
fill = I('blue'),
alpha = 0.6)+
labs(title = 'Frekuensi Kata',
subtitle = 'Takbiran',
caption = 'Twitter Crawling 1 Mei 2022')+
xlab('Kata')+
ylab('Jumlah')+
coord_flip()+
theme(axis.text = element_text(size = 9))+
theme_bw()
reticulate::repl_python()
import pandas as pd
from sklearn.model_selection import train_test_split
import joblib
from sklearn.feature_extraction.text import CountVectorizer
data = pd.read_csv("C:\\Users\\fkhas\\Documents\\Projects\\Twitter\\SA\\tugasakhirfix\\Datasets\\Sentiment Takbiran.csv")
data.head()
import re
from nltk.corpus import stopwords
def preprocess_data(data):
# Remove package name as it's not relevant
data = data.drop('score', axis=1)
# Convert text to lowercase
data['text'] = data['text'].str.strip().str.lower()
return data
data = preprocess_data(data)
# Split into training and testing data
x = data['text']
y = data['class']
x, x_test, y, y_test = train_test_split(x,y, stratify=y, test_size=0.25, random_state=42)
# Vectorize text reviews to numbers
vec = CountVectorizer(stop_words='indonesia')
x = vec.fit_transform(x).toarray()
x_test = vec.transform(x_test).toarray()
from sklearn.naive_bayes import MultinomialNB
model = MultinomialNB()
model.fit(x, y)
model.score(x_test, y_test)
model.predict(vec.transform(['saya benci kamu!']))
View(data)
import pandas as pd
from sklearn.model_selection import train_test_split
import joblib
from sklearn.feature_extraction.text import CountVectorizer
data = pd.read_csv("C:\\Users\\fkhas\\Documents\\Projects\\Twitter\\SA\\tugasakhirfix\\Datasets\\Sentiment Takbiran.csv")
data.head()
import re
from nltk.corpus import stopwords
def preprocess_data(data):
# Remove package name as it's not relevant
data = data.drop('score', axis=1)
# Convert text to lowercase
data['text'] = data['text'].str.strip().str.lower()
return data
data = preprocess_data(data)
# Split into training and testing data
x = data['text']
y = data['class']
x, x_test, y, y_test = train_test_split(x,y, stratify=y, test_size=0.25, random_state=42)
# Vectorize text reviews to numbers
vec = CountVectorizer(stop_words='english')
x = vec.fit_transform(x).toarray()
x_test = vec.transform(x_test).toarray()
from sklearn.naive_bayes import MultinomialNB
model = MultinomialNB()
model.fit(x, y)
model.score(x_test, y_test)
model.predict(vec.transform(['saya benci kamu!']))
model.predict(vec.transform(['saya sayang kamu!']))
model.predict(vec.transform(['saya mencintai kamu!']))
model.predict(vec.transform(['saya sangat senang dan bahagia karena kamu!']))
