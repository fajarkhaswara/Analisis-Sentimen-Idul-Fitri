# load needed packages
library(dplyr)
library(tidyr)
library(tidytext)
library(tm)
library(dplyr)
library(stringr)
library(wordcloud)
library(ggplot2)
library(rtweet)

# collect twitter data
df <- search_tweets(
  "shalat id", n = 5000, include_rts = TRUE, lang = "id", type = "recent"
)

#flaten the tweets
flatenned_tweets <- data.frame(lapply(df, as.character), stringsAsFactors = FALSE)

#save raw twitter data
write.csv(x = flatenned_tweets,
          file = '~/Projects/Twitter/SA/analisissentimen/Datasets/Data Shalat Id Mentah.csv',
          row.names = FALSE)

# remove duplicate and select single column for only text
tweets.df = df %>% distinct(text, .keep_all = FALSE)

#save twitter data
write.csv(x = tweets.df,
          file = '~/Projects/Twitter/SA/analisissentimen/Datasets/Data Shalat Id.csv',
          row.names = FALSE)

# Load Data from Computer
tweets.df = read.csv(file = '~/Projects/Twitter/SA/analisissentimen/Datasets/Data Shalat Id.csv',
                     header = TRUE,
                     sep = ',')

# ===== DATA CLEANSING
# Work with Corpus
tweet.corpus = VCorpus(VectorSource(tweets.df$text))
inspect(tweet.corpus[[1]])

# Import external function
source(file = '~/Projects/Twitter/SA/analisissentimen/Helpers/Function Helper/Cleansing.R')

# TRANSFORM TO LOWER CASE
tweet.corpus = tm_map(tweet.corpus, content_transformer(tolower))
# FILTERING - CUSTOM CLEANSING FUNCTIONS

tweet.corpus = tm_map(tweet.corpus, content_transformer(removeURL))
tweet.corpus = tm_map(tweet.corpus, content_transformer(unescapeHTML))
tweet.corpus = tm_map(tweet.corpus, content_transformer(removeMention))
tweet.corpus = tm_map(tweet.corpus, content_transformer(removeCarriage))
tweet.corpus = tm_map(tweet.corpus, content_transformer(removeEmoticon))
tweet.corpus = tm_map(tweet.corpus, content_transformer(removeInvoice))

# Remove additional symbols to white space
tweet.corpus = tm_map(tweet.corpus, toSpace, "[[:punct:]]") # punctuation
tweet.corpus = tm_map(tweet.corpus, toSpace, "[[:digit:]]") # numbers

# Eliminate extra white spaces
tweet.corpus = tm_map(tweet.corpus, stripWhitespace)

# Check the final result
inspect(tweet.corpus[[1]])

# SPELLING NORMALIZATION
spell.lex = read.csv(file = '~/Projects/Twitter/SA/analisissentimen/Helpers/Data Helper/colloquial-indonesian-lexicon.txt',
                     header = TRUE,
                     sep = ',',
                     stringsAsFactors = FALSE)

# Import external function
source(file = '~/Projects/Twitter/SA/analisissentimen/Helpers/Function Helper/Cleansing.R')
tweet.corpus = tm_map(tweet.corpus, spell.correction, spell.lex)

# STEMMING WORDS
# Import external function
source(file = '~/Projects/Twitter/SA/analisissentimen/Helpers/Function Helper/Cleansing.R')
tweet.corpus = tm_map(tweet.corpus, content_transformer(stemming))


# ===== LEXICON-BASED SENTIMENT SCORING
pos = readLines('~/Projects/Twitter/SA/analisissentimen/Helpers/Data Helper/s-pos.txt')
neg = readLines('~/Projects/Twitter/SA/analisissentimen/Helpers/Data Helper/s-neg.txt')
df.tweet = data.frame(text = sapply(tweet.corpus, as.character),
                      stringsAsFactors = FALSE)


# Negation Handling
negasi = scan('~/Projects/Twitter/SA/analisissentimen/Helpers/Data Helper/negatingword.txt',
              what = 'character')
senti = read.csv('~/Projects/Twitter/SA/analisissentimen/Helpers/Data Helper/sentiwords_id.txt',
                 sep = ':', 
                 header = FALSE) %>% 
  mutate(words = as.character(V1),
         score = as.numeric(V2)) %>% 
  select(c('words','score'))
booster = read.csv('~/Projects/Twitter/SA/analisissentimen/Helpers/Data Helper/boosterwords_id.txt',
                   sep = ":", 
                   header = FALSE) %>%
  mutate(words = as.character(V1),
         score = as.numeric(V2)) %>% 
  select(c('words','score'))
# Import external function
source(file = '~/Projects/Twitter/SA/analisissentimen/Helpers/Function Helper/Lexicon-Based Scoring Analysis.R')
results = scores.sentiment(df.tweet$text, senti, negasi)

set.seed(2211)
random.tweet = sample(x = df.tweet$text, 
                      size = 10)
head(scores.sentiment(random.tweet, senti, negasi),5)
# Convert score to sentiment classes
results$class = as.factor(ifelse(results$score < 0, 'Negative',
                                 ifelse(results$score == 0, 'Neutral','Positive')))

# ===== SAVE DATA RESULTS
write.csv(x = results,
          file = '~/Projects/Twitter/SA/analisissentimen/Datasets/Sentiment Shalat Id.csv',
          row.names = FALSE)